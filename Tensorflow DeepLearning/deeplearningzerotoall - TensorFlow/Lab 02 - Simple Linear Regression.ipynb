{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모두를 위한 딥러닝 시즌2 - TensorFlow\n",
    "\n",
    "## LAB02 - Simple Linear Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#간단한 데이터를 만들었다\n",
    "x_data = [1,2,3,4,5]\n",
    "y_data = [1,2,3,4,5]\n",
    "\n",
    "#초기값을 설정했음.\n",
    "W=tf.Variable(2.9)\n",
    "b=tf.Variable(0.5)\n",
    "\n",
    "#hypothesis = W*x+b\n",
    "hypothesis = W*x_data+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cost function\n",
    "cost = tf.reduce_mean(tf.square(hypothesis-y_data))\n",
    "#tf.reduce_mean : 차원(rank)이 하나 줄어들면서 평균을 구한다.\n",
    "#tf.square() 은 그냥 제곱 함수!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cost를 Minimize 하는 알고리즘 중 가장 유명한게 Gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0|     2.452|     0.376| 45.660004\n",
      "   10|     1.104|  0.003398|  0.206336\n",
      "   20|     1.013|  -0.02091|  0.001026\n",
      "   30|     1.007|  -0.02184|  0.000093\n",
      "   40|     1.006|  -0.02123|  0.000083\n",
      "   50|     1.006|  -0.02053|  0.000077\n",
      "   60|     1.005|  -0.01984|  0.000072\n",
      "   70|     1.005|  -0.01918|  0.000067\n",
      "   80|     1.005|  -0.01854|  0.000063\n",
      "   90|     1.005|  -0.01793|  0.000059\n"
     ]
    }
   ],
   "source": [
    "#학습률 초기화\n",
    "learning_rate = 0.01\n",
    "\n",
    "#gradient descent 100번 시행해줌.\n",
    "for i in range(100):\n",
    "    #Gradient descent\n",
    "    #with 구문 안 변수들의 변화의 정보를 tape에 기록\n",
    "    #여기엔 W, b 가 있음.\n",
    "    with tf.GradientTape() as tape:\n",
    "        hypothesis = W*x_data +b\n",
    "        cost = tf.reduce_mean(tf.square(hypothesis - y_data))\n",
    "    #tape의 gradient 메소드를 호출해 미분값을 구함. 변수들의 개별 미분값(기울기)\n",
    "    W_grad, b_grad = tape.gradient(cost,[W,b])\n",
    "    #W, b값을 업데이트 해줌.\n",
    "    #A = A-B, A-=B 를 텐서에 사용할 수 없어서 이 메소드를 이용하는 것\n",
    "    W.assign_sub(learning_rate *W_grad)\n",
    "    b.assign_sub(learning_rate*b_grad)\n",
    "    \n",
    "    if i%10==0:\n",
    "        print(\"{:5}|{:10.4}|{:10.4}|{:10.6f}\".format(i,W.numpy(), b.numpy(), cost))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
