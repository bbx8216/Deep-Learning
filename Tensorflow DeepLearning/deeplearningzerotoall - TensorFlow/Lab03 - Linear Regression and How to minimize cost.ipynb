{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모두를 위한 딥러닝 시즌2 - TensorFlow\n",
    "\n",
    "## LAB03 - Linear Regression and How to minimze cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost function in pure Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3.000|  74.66667\n",
      "-2.429|  54.85714\n",
      "-1.857|  38.09524\n",
      "-1.286|  24.38095\n",
      "-0.714|  13.71429\n",
      "-0.143|   6.09524\n",
      " 0.429|   1.52381\n",
      " 1.000|   0.00000\n",
      " 1.571|   1.52381\n",
      " 2.143|   6.09524\n",
      " 2.714|  13.71429\n",
      " 3.286|  24.38095\n",
      " 3.857|  38.09524\n",
      " 4.429|  54.85714\n",
      " 5.000|  74.66667\n"
     ]
    }
   ],
   "source": [
    "X=np.array([1,2,3])\n",
    "Y=np.array([1,2,3])\n",
    "\n",
    "#데이터 X,Y에 대해서 W가 주어졌을 때 cost를 계산하는 함수\n",
    "def cost_func(W,X,Y):\n",
    "    c=0\n",
    "    for i in range(len(X)):\n",
    "        # c = 편차 제곱을 다 더한 것\n",
    "        c += (W*X[i]-Y[i])**2\n",
    "    #편차 제곱의 평균\n",
    "    return c/len(X)\n",
    "\n",
    "for feed_W in np.linspace(-3,5,num=15): #np.linspace(여기부터, 여기까지, 이만큼의 구간)을 가짐\n",
    "    curr_cost = cost_func(feed_W, X, Y)\n",
    "    print(\"{:6.3f}|{:10.5f}\".format(feed_W, curr_cost))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost function in TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3.000|  74.66667\n",
      "-2.429|  54.85714\n",
      "-1.857|  38.09524\n",
      "-1.286|  24.38095\n",
      "-0.714|  13.71429\n",
      "-0.143|   6.09524\n",
      " 0.429|   1.52381\n",
      " 1.000|   0.00000\n",
      " 1.571|   1.52381\n",
      " 2.143|   6.09524\n",
      " 2.714|  13.71429\n",
      " 3.286|  24.38095\n",
      " 3.857|  38.09524\n",
      " 4.429|  54.85714\n",
      " 5.000|  74.66667\n"
     ]
    }
   ],
   "source": [
    "X = np.array([1,2,3])\n",
    "Y = np.array([1,2,3])\n",
    "\n",
    "def cost_func(W,X,Y):\n",
    "    hypothesis = X*W\n",
    "    return tf.reduce_mean(tf.square(hypothesis-Y))\n",
    "\n",
    "#쪼갠 값을 리스트로 받는다.\n",
    "W_values = np.linspace(-3,5,num=15)\n",
    "cost_values = []\n",
    "\n",
    "#\n",
    "for feed_W in W_values:\n",
    "    curr_cost = cost_func(feed_W, X, Y)\n",
    "    cost_values.append(curr_cost)\n",
    "    print(\"{:6.3f}|{:10.5f}\".format(feed_W,curr_cost))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient descent를 tensorflow cost function으로"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'W' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-230796d511ea>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0malpha\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.01\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mgradient\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m#새로운 W값\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdescent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mW\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#W에 새로운 W값(descent)할당\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'W' is not defined"
     ]
    }
   ],
   "source": [
    "alpha = 0.01\n",
    "gradient = tf.reduce_mean(tf.multiply(tf.multiply(W,X)-Y,X))\n",
    "#새로운 W값\n",
    "descent = W - tf.multiply(alpha, gradient)\n",
    "#W에 새로운 W값(descent)할당\n",
    "W.assign(descent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0|11716.3086| 48.767971\n",
      "   10| 4504.9126| 30.619968\n",
      "   20| 1732.1364| 19.366755\n",
      "   30|  666.0052| 12.388859\n",
      "   40|  256.0785|  8.062004\n",
      "   50|   98.4620|  5.379007\n",
      "   60|   37.8586|  3.715335\n",
      "   70|   14.5566|  2.683725\n",
      "   80|    5.5970|  2.044044\n",
      "   90|    2.1520|  1.647391\n",
      "  100|    0.8275|  1.401434\n",
      "  110|    0.3182|  1.248922\n",
      "  120|    0.1223|  1.154351\n",
      "  130|    0.0470|  1.095710\n",
      "  140|    0.0181|  1.059348\n",
      "  150|    0.0070|  1.036801\n",
      "  160|    0.0027|  1.022819\n",
      "  170|    0.0010|  1.014150\n",
      "  180|    0.0004|  1.008774\n",
      "  190|    0.0002|  1.005441\n",
      "  200|    0.0001|  1.003374\n",
      "  210|    0.0000|  1.002092\n",
      "  220|    0.0000|  1.001297\n",
      "  230|    0.0000|  1.000804\n",
      "  240|    0.0000|  1.000499\n",
      "  250|    0.0000|  1.000309\n",
      "  260|    0.0000|  1.000192\n",
      "  270|    0.0000|  1.000119\n",
      "  280|    0.0000|  1.000074\n",
      "  290|    0.0000|  1.000046\n"
     ]
    }
   ],
   "source": [
    "#random seed 초기화 -> 다음에 이 코드를 다시 수행했을 때도 똑같이 재현되게 하기위해서\n",
    "tf.random.set_seed(0)\n",
    "x_data = [1.,2.,3.,4.]\n",
    "y_data = [1.,3.,5.,7.]\n",
    " \n",
    "#정규분포를 따르는 랜덤넘버를 한개 짜리로 변수를 만들어서 W에 할당\n",
    "W = tf.Variable(tf.random.normal([1],-100.,100.))\n",
    "\n",
    "for step in range(300):\n",
    "    hypothesis = W*X\n",
    "    cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "    \n",
    "    alpha = 0.01\n",
    "    gradient = tf.reduce_mean(tf.multiply(tf.multiply(W,X)-Y,X))\n",
    "    descent = W - tf.multiply(alpha, gradient)\n",
    "    W.assign(descent)\n",
    "    \n",
    "    if step%10==0:\n",
    "        print('{:5}|{:10.4f}|{:10.6f}'.format(\n",
    "        step, cost.numpy(), W.numpy()[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
